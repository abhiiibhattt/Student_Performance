{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69a4fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 'DL(Term_Paper).csv' with 50000 records.\n",
      "This dataset has stronger correlations and should yield higher model accuracy.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def generate_high_accuracy_dataset(num_students=50000):\n",
    "    \"\"\"\n",
    "    Generates a large-scale synthetic dataset with strong correlations\n",
    "    to facilitate higher model accuracy.\n",
    "\n",
    "    Args:\n",
    "        num_students (int): The number of student records to generate.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the synthetic student data.\n",
    "    \"\"\"\n",
    "    universities = [\n",
    "        'IIT Madras', 'IISc Bangalore', 'NIT Tiruchirappalli',\n",
    "        'Vellore Institute of Technology', 'Amrita Vishwa Vidyapeetham',\n",
    "        'University of Hyderabad', 'NIT Surathkal', 'Anna University'\n",
    "    ]\n",
    "    university_weights = {\n",
    "        'IIT Madras': 1.05, 'IISc Bangalore': 1.06, 'NIT Tiruchirappalli': 1.03,\n",
    "        'Vellore Institute of Technology': 1.0, 'Amrita Vishwa Vidyapeetham': 0.98,\n",
    "        'University of Hyderabad': 0.99, 'NIT Surathkal': 1.02, 'Anna University': 0.97\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in range(num_students):\n",
    "        student_id = f'SID{i+1:05d}'\n",
    "        university = random.choice(universities)\n",
    "        weight = university_weights[university]\n",
    "        \n",
    "        # Base performance factor is now more deterministic\n",
    "        base_performance = np.random.normal(0.9, 0.15) * weight\n",
    "        base_performance = np.clip(base_performance, 0.4, 1.2)\n",
    "        \n",
    "        prior_cgpa = np.random.uniform(5.0, 9.8) * base_performance\n",
    "        prior_cgpa = np.clip(prior_cgpa, 5.0, 10.0)\n",
    "        \n",
    "        # Engagement metrics are tightly coupled with base performance\n",
    "        time_spent = round(np.random.uniform(1, 15) * base_performance, 1)\n",
    "        login_frequency = int(time_spent / 1.5 + np.random.uniform(0, 2))\n",
    "        discussion_posts = int(time_spent * 2.5 * base_performance)\n",
    "        resource_access = int(time_spent * 5 * base_performance)\n",
    "        \n",
    "        assignment_completion = int(np.random.uniform(60, 100) * base_performance)\n",
    "        avg_quiz_score = int(assignment_completion * np.random.uniform(0.9, 1.05) * base_performance)\n",
    "        midterm_score = int((avg_quiz_score * 0.5 + assignment_completion * 0.5) * np.random.uniform(0.95, 1.05))\n",
    "\n",
    "        # Clipping scores to ensure they are within the valid 0-100 range\n",
    "        avg_quiz_score = np.clip(avg_quiz_score, 0, 100)\n",
    "        assignment_completion = np.clip(assignment_completion, 0, 100)\n",
    "        midterm_score = np.clip(midterm_score, 0, 100)\n",
    "        \n",
    "        # Target Variable: Final Grade with stronger weights and less noise\n",
    "        final_grade = int(\n",
    "            prior_cgpa * 2.0 +\n",
    "            time_spent * 0.5 +\n",
    "            avg_quiz_score * 0.3 +\n",
    "            assignment_completion * 0.2 +\n",
    "            midterm_score * 0.4 +\n",
    "            np.random.normal(0, 1.0) # Reduced noise from 2.5 to 1.0\n",
    "        )\n",
    "        final_grade = np.clip(final_grade, 38, 100)\n",
    "\n",
    "        data.append({\n",
    "            'student_id': student_id,\n",
    "            'university_name': university,\n",
    "            'login_frequency_per_week': login_frequency,\n",
    "            'time_spent_hours_per_week': time_spent,\n",
    "            'discussion_posts_per_semester': discussion_posts,\n",
    "            'resource_access_per_week': resource_access,\n",
    "            'avg_quiz_score': avg_quiz_score,\n",
    "            'assignment_completion_rate': assignment_completion,\n",
    "            'midterm_score': midterm_score,\n",
    "            'prior_cgpa': round(prior_cgpa, 2),\n",
    "            'final_grade': final_grade\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "number_of_records = 50000 \n",
    "\n",
    "high_acc_df = generate_high_accuracy_dataset(num_students=number_of_records)\n",
    "\n",
    "output_filename = 'DL(Term_Paper).csv'\n",
    "high_acc_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Successfully generated '{output_filename}' with {len(high_acc_df)} records.\")\n",
    "print(\"This dataset has stronger correlations and should yield higher model accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1cf7b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhii\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "ðŸš€ Starting: Loading and preprocessing data...\n",
      "âœ… Preprocessing complete.\n",
      "\n",
      "ðŸ¤– Training Baseline Models...\n",
      "   - Training Random Forest...\n",
      "   -> Random Forest finished. R2: 0.9962, RMSE: 1.0562\n",
      "   - Training XGBoost...\n",
      "   -> XGBoost finished. R2: 0.9964, RMSE: 1.0244\n",
      "\n",
      "ðŸ§  Training Deep Learning Models...\n",
      "   - Building and training LSTM model...\n",
      "WARNING:tensorflow:From C:\\Users\\abhii\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\abhii\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\abhii\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "   -> LSTM training complete.\n",
      "313/313 [==============================] - 6s 6ms/step\n",
      "   -> LSTM finished. R2: 0.9966, RMSE: 1.0050\n",
      "   - Building and training Transformer model...\n",
      "   -> Transformer training complete.\n",
      "313/313 [==============================] - 3s 6ms/step\n",
      "   -> Transformer finished. R2: 0.9954, RMSE: 1.1588\n",
      "\n",
      "ðŸ“Š Generating and Saving Figures...\n",
      "\n",
      "--- Final Model Performance Summary ---\n",
      "                   R2    RMSE\n",
      "Random Forest  0.9962  1.0562\n",
      "XGBoost        0.9964  1.0244\n",
      "LSTM           0.9966  1.0050\n",
      "Transformer    0.9954  1.1588\n",
      "---------------------------------------\n",
      "âœ… Saved 'rmse_chart.png' to 'figures/' folder.\n",
      "âœ… Saved 'feature_importance.png' to 'figures/' folder.\n",
      "âœ… Saved 'loss_curves.png' to 'figures/' folder.\n",
      "\n",
      "ðŸŽ‰ Script finished successfully! You're all set.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Term Paper Implementation: Predicting Student Success\n",
    "# ==============================================================================\n",
    "# This script performs the following actions:\n",
    "# 1. Loads the 'DL(Term_Paper).csv' dataset.\n",
    "# 2. Preprocesses the data as described in the paper.\n",
    "# 3. Trains and evaluates Random Forest, XGBoost, LSTM, and Transformer models.\n",
    "# 4. Prints a summary of the results.\n",
    "# 5. Generates and saves the figures required for the LaTeX paper.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Scikit-learn for preprocessing and baseline models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# TensorFlow and Keras for Deep Learning models\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_FILENAME = 'DL(Term_Paper).csv'\n",
    "FIGURES_DIR = 'figures'\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 1: DATA LOADING AND PREPROCESSING\n",
    "# ==============================================================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Loads and preprocesses the data according to the paper's methodology.\"\"\"\n",
    "    print(\"ðŸš€ Starting: Loading and preprocessing data...\")\n",
    "    \n",
    "    if not os.path.exists(DATASET_FILENAME):\n",
    "        print(f\"Error: Dataset file '{DATASET_FILENAME}' not found!\")\n",
    "        print(\"Please make sure the dataset is in the same directory as this script.\")\n",
    "        exit()\n",
    "        \n",
    "    df = pd.read_csv(DATASET_FILENAME)\n",
    "    \n",
    "    # One-Hot Encoding for the 'university_name' column\n",
    "    df = pd.get_dummies(df, columns=['university_name'], prefix='uni')\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.drop(columns=['student_id', 'final_grade'])\n",
    "    y = df['final_grade']\n",
    "    \n",
    "    # Split data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale numerical features using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert scaled arrays back to DataFrames to keep column names\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    print(\"âœ… Preprocessing complete.\")\n",
    "    return X_train_scaled_df, X_test_scaled_df, y_train, y_test\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: BASELINE MODELS (RANDOM FOREST & XGBOOST)\n",
    "# ==============================================================================\n",
    "\n",
    "def train_baseline_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains and evaluates the Random Forest and XGBoost models.\"\"\"\n",
    "    print(\"\\nðŸ¤– Training Baseline Models...\")\n",
    "    results = {}\n",
    "    \n",
    "    # --- Random Forest ---\n",
    "    print(\"   - Training Random Forest...\")\n",
    "    start_time = time.time()\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    r2_rf = r2_score(y_test, y_pred_rf)\n",
    "    rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "    results['Random Forest'] = {'R2': r2_rf, 'RMSE': rmse_rf}\n",
    "    print(f\"   -> Random Forest finished. R2: {r2_rf:.4f}, RMSE: {rmse_rf:.4f}\")\n",
    "\n",
    "    # --- XGBoost ---\n",
    "    print(\"   - Training XGBoost...\")\n",
    "    start_time = time.time()\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "    results['XGBoost'] = {'R2': r2_xgb, 'RMSE': rmse_xgb}\n",
    "    print(f\"   -> XGBoost finished. R2: {r2_xgb:.4f}, RMSE: {rmse_xgb:.4f}\")\n",
    "    \n",
    "    # Extract feature importances for later plotting\n",
    "    feature_importances = pd.Series(xgb_model.feature_importances_, index=X_train.columns)\n",
    "    \n",
    "    return results, feature_importances\n",
    "    \n",
    "# ==============================================================================\n",
    "# SECTION 3: DEEP LEARNING MODELS (LSTM & TRANSFORMER)\n",
    "# ==============================================================================\n",
    "\n",
    "def train_deep_learning_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Defines, trains, and evaluates the LSTM and Transformer models.\"\"\"\n",
    "    print(\"\\nðŸ§  Training Deep Learning Models...\")\n",
    "    results = {}\n",
    "    \n",
    "    # Reshape data for DL models: (samples, timesteps, features)\n",
    "    # For tabular data, we have 1 timestep.\n",
    "    X_train_reshaped = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test_reshaped = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    # --- LSTM Model ---\n",
    "    print(\"   - Building and training LSTM model...\")\n",
    "    lstm_model = keras.Sequential([\n",
    "        layers.Input(shape=(1, X_train.shape[1])),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(64),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    history_lstm = lstm_model.fit(\n",
    "        X_train_reshaped, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        verbose=0 # Set to 1 to see live training progress\n",
    "    )\n",
    "    \n",
    "    print(\"   -> LSTM training complete.\")\n",
    "    y_pred_lstm = lstm_model.predict(X_test_reshaped).flatten()\n",
    "    r2_lstm = r2_score(y_test, y_pred_lstm)\n",
    "    rmse_lstm = np.sqrt(mean_squared_error(y_test, y_pred_lstm))\n",
    "    results['LSTM'] = {'R2': r2_lstm, 'RMSE': rmse_lstm}\n",
    "    print(f\"   -> LSTM finished. R2: {r2_lstm:.4f}, RMSE: {rmse_lstm:.4f}\")\n",
    "    \n",
    "    # --- Transformer Model ---\n",
    "    def transformer_encoder(inputs):\n",
    "        x = layers.MultiHeadAttention(key_dim=64, num_heads=4, dropout=0.1)(inputs, inputs)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(inputs + x)\n",
    "        ff_net = keras.Sequential([layers.Dense(32, activation=\"relu\"), layers.Dense(inputs.shape[-1]),])\n",
    "        x_ff = ff_net(x)\n",
    "        return layers.LayerNormalization(epsilon=1e-6)(x + x_ff)\n",
    "\n",
    "    print(\"   - Building and training Transformer model...\")\n",
    "    inputs = layers.Input(shape=(1, X_train.shape[1]))\n",
    "    x = transformer_encoder(inputs)\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    transformer_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    transformer_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    transformer_model.fit(\n",
    "        X_train_reshaped, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"   -> Transformer training complete.\")\n",
    "    y_pred_transformer = transformer_model.predict(X_test_reshaped).flatten()\n",
    "    r2_transformer = r2_score(y_test, y_pred_transformer)\n",
    "    rmse_transformer = np.sqrt(mean_squared_error(y_test, y_pred_transformer))\n",
    "    results['Transformer'] = {'R2': r2_transformer, 'RMSE': rmse_transformer}\n",
    "    print(f\"   -> Transformer finished. R2: {r2_transformer:.4f}, RMSE: {rmse_transformer:.4f}\")\n",
    "    \n",
    "    return results, history_lstm\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 4: RESULTS AGGREGATION AND VISUALIZATION\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_visualizations(all_results, feature_importances, history_lstm):\n",
    "    \"\"\"Generates and saves all the figures for the paper.\"\"\"\n",
    "    print(\"\\nðŸ“Š Generating and Saving Figures...\")\n",
    "    \n",
    "    if not os.path.exists(FIGURES_DIR):\n",
    "        os.makedirs(FIGURES_DIR)\n",
    "        \n",
    "    # --- 1. Performance Table (Printed to Console) ---\n",
    "    results_df = pd.DataFrame(all_results).T\n",
    "    print(\"\\n--- Final Model Performance Summary ---\")\n",
    "    print(results_df[['R2', 'RMSE']].round(4))\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    # --- 2. RMSE Bar Chart ---\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    models = list(all_results.keys())\n",
    "    rmse_values = [res['RMSE'] for res in all_results.values()]\n",
    "    \n",
    "    colors = ['#4c72b0', '#55a868', '#c44e52', '#8172b2']\n",
    "    bars = ax.bar(models, rmse_values, color=colors)\n",
    "    ax.set_ylabel('Root Mean Squared Error (RMSE)')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_title('Model Performance Comparison (RMSE)')\n",
    "    ax.bar_label(bars, fmt='%.4f')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'rmse_chart.png'))\n",
    "    print(f\"âœ… Saved 'rmse_chart.png' to '{FIGURES_DIR}/' folder.\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- 3. Feature Importance Plot ---\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    feature_importances.sort_values(ascending=True).plot(kind='barh', ax=ax, color='#55a868')\n",
    "    ax.set_xlabel('Feature Importance Score (XGBoost)')\n",
    "    ax.set_title('Feature Importance Analysis')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'feature_importance.png'))\n",
    "    print(f\"âœ… Saved 'feature_importance.png' to '{FIGURES_DIR}/' folder.\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- 4. Deep Learning Loss Curves ---\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(history_lstm.history['loss'], label='Training Loss', color='#c44e52')\n",
    "    ax.plot(history_lstm.history['val_loss'], label='Validation Loss', color='#4c72b0')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Mean Squared Error (Loss)')\n",
    "    ax.set_title('LSTM Model Training and Validation Loss')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'loss_curves.png'))\n",
    "    print(f\"âœ… Saved 'loss_curves.png' to '{FIGURES_DIR}/' folder.\")\n",
    "    plt.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Step 1: Load and preprocess data\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data()\n",
    "    \n",
    "    # Step 2: Train and evaluate baseline models\n",
    "    baseline_results, feature_importances = train_baseline_models(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Step 3: Train and evaluate deep learning models\n",
    "    dl_results, history_lstm = train_deep_learning_models(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Step 4: Combine results and generate all plots\n",
    "    all_results = {**baseline_results, **dl_results}\n",
    "    generate_visualizations(all_results, feature_importances, history_lstm)\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Script finished successfully! You're all set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973e574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
